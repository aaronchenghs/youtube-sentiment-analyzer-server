# -*- coding: utf-8 -*-
"""YoutubeComment_Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jn_n7zq2pI5Wvf46DptRU6VHyQ4kcq6T
"""

from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import joblib

# To Train and evaluate a model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline as make_pipeline_imb

from constants import credentials_path, SCOPES, redirect_uri

flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES, redirect_uri=redirect_uri )
auth_url, _ = flow.authorization_url(access_type='offline')

print('Please go to this URL and authorize access:', auth_url)

"""** Your Google Account must be manually authorized by a developer to make the API connection. Please contact Aaron, Duc, or Henry for perms. **"""

auth_code = input('Enter the authorization code: ')

# Fetch the token without specifying redirect_uri again
flow.fetch_token(code=auth_code)

credentials = flow.credentials
youtube = build('youtube', 'v3', credentials=credentials)

with open('./api_key.txt', 'r') as file:
    api_key = file.read().strip()


def extract_video_id(url):
    # Extract the video ID from the YouTube URL
    import re
    regex = r"(?<=v=)[^&#]+"
    matches = re.search(regex, url)
    if matches:
        return matches.group(0)
    else:
        regex = r"(?<=be/)[^&#]+"
        matches = re.search(regex, url)
        return matches.group(0) if matches else None


def fetch_comments(youtube, video_id):
    comments = []
    next_page_token = None

    while len(comments) < 1000:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,  # Batch by 100 comments
            pageToken=next_page_token,
            textFormat="plainText"
        )
        response = request.execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)
            if len(comments) >= 1000:
                break

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break

    return comments

"""Use the csv file from this dataset: https://www.kaggle.com/datasets/reihanenamdari/youtube-toxicity-data"""

def train_and_evaluate_model(classifier, train_data, test_data):
    # Create a pipeline with TfidfVectorizer and SMOTE for handling class imbalance
    pipeline = make_pipeline_imb(
        TfidfVectorizer(),
        SMOTE(random_state=42),
        classifier
    )

    # Set up the grid search parameters
    parameters = {
        'tfidfvectorizer__max_df': (0.75, 0.85),
        'tfidfvectorizer__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
        # Add other hyperparameters here
    }

    grid_search = GridSearchCV(pipeline, parameters, cv=5)
    grid_search.fit(train_data['Text'], train_data['IsToxic'])

    # Best model after grid search
    model = grid_search.best_estimator_

    # Evaluate the model
    predictions = model.predict(test_data['Text'])
    accuracy = accuracy_score(test_data['IsToxic'], predictions)
    print(f"{classifier.__class__.__name__} Accuracy:", accuracy)

    return model, accuracy

# Upload and read data
data = pd.read_csv('./youtoxic_english_1000.csv')

# Splitting the data
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Define classifiers
classifiers = [
    MultinomialNB(),
]

# Train and evaluate models, and keep track of accuracies
models = {}
accuracy_scores = {}
for classifier in classifiers:
    model_name = classifier.__class__.__name__
    model, accuracy = train_and_evaluate_model(classifier, train_data, test_data)
    models[model_name] = model
    accuracy_scores[model_name] = accuracy

# Identify and save the model with the highest accuracy
# Replace best_model_name value with 'LogisticRegression', 'RandomForestClassifier', or 'DecisionTreeClassifier'
# To select a model
best_model_name = max(accuracy_scores, key=accuracy_scores.get)
print(f"\nSaving the best model: {best_model_name} with accuracy {accuracy_scores[best_model_name]:.2f}")
joblib.dump(models[best_model_name], 'sentiment_model.joblib')

# Function to analyze sentiment
def analyze_sentiment(comment, trained_model):
    prediction = trained_model.predict([comment])
    return prediction[0]

"""Replace best_model_name value with 'LogisticRegression', 'RandomForestClassifier', or 'DecisionTreeClassifier'
To select a model

Jake Paul vs. Floyd Mayweather Boxing Match: https://www.youtube.com/watch?v=9gFiNPl_-DQ&ab_channel=That%27swhyMMA%21

News on War in the Middle East: https://www.youtube.com/watch?v=IiLJPkHFkYI&ab_channel=NBCNews

Chill Lofi Beats Video: https://www.youtube.com/watch?v=i43tkaTXtwI&ab_channel=LofiGirl

Mr Beast Helping Hurricane Survivors: https://www.youtube.com/watch?v=VcvQ02eRhM4&ab_channel=BeastPhilanthropy
"""

user_url = input("Please enter a YouTube video URL: ")

video_id = extract_video_id(user_url)

# Assuming 'youtube' is an authenticated YouTube API client
comments = fetch_comments(youtube, video_id)

# Load the trained sentiment model
trained_model = joblib.load('sentiment_model.joblib')

positive_count = 0
negative_count = 0

# Analyze sentiments for each comment
for comment in comments:
    sentiment = analyze_sentiment(comment, trained_model)
    if sentiment == 0:
        positive_count += 1
    else:
        negative_count += 1

# Calculate overall sentiment ratio or percentage
total_comments = len(comments)
positive_ratio = positive_count / total_comments
negative_ratio = negative_count / total_comments

# Print the overall sentiment rating
print(f"Most recent {total_comments} comments analyzed.")
print(f"{positive_count} comments determined to have POSITIVE sentiment.")
print(f"{negative_count} comments determined to have NEGATIVE sentiment.")
print(f"Overall Sentiment Rating: Positive - {positive_ratio * 100:.2f}%, Negative - {negative_ratio * 100:.2f}%")